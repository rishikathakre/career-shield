# ==========================================
# 1. INSTALL & SETUP
# ==========================================
print("Installing libraries...")
!pip install -q transformers accelerate evaluate torch scikit-learn pandas

import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.utils.class_weight import compute_class_weight

# Check for GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ==========================================
# 2. LOAD & PROCESS DATA
# ==========================================
print("Loading data...")
try:
    df = pd.read_csv('fake_job_postings.csv')
except FileNotFoundError:
    print("ERROR: File not found. Please drag 'fake_job_postings.csv' into the Files sidebar!")
    raise

# Clean and Combine Text
df.fillna(" ", inplace=True)
df['text'] = df['title'] + " " + df['description'] + " " + df['requirements']

# Split Data (Stratified to keep the 5% fake ratio consistent)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['fraudulent'].tolist(),
    test_size=0.2,
    stratify=df['fraudulent'],
    random_state=42
)

# ==========================================
# 3. TOKENIZATION
# ==========================================
print("Tokenizing... (This takes ~30 seconds)")
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize_data(texts):
    return tokenizer(texts, truncation=True, padding=True, max_length=128)

train_encodings = tokenize_data(train_texts)
val_encodings = tokenize_data(val_texts)

class JobDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = JobDataset(train_encodings, train_labels)
val_dataset = JobDataset(val_encodings, val_labels)

# ==========================================
# 4. WEIGHTED TRAINER SETUP
# ==========================================
# Calculate weights to handle class imbalance
weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(train_labels),
    y=train_labels
)
class_weights = torch.tensor(weights, dtype=torch.float).to(device)

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    return {
        "accuracy": accuracy_score(labels, pred),
        "precision": precision_score(labels, pred),
        "recall": recall_score(labels, pred),
        "f1": f1_score(labels, pred)
    }

# ==========================================
# 5. TRAIN
# ==========================================
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)
model.to(device)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    learning_rate=2e-5,
    weight_decay=0.01,
    eval_strategy="epoch",  # <--- UPDATED THIS LINE
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none"
)

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

print("Starting Training... (Relax, this will take 10-15 mins)")
trainer.train()

# ==========================================
# 6. SAVE & DOWNLOAD
# ==========================================
print("Saving model...")
save_path = "./fake_job_model"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("Zipping model for download...")
!zip -r fake_job_model.zip ./fake_job_model

from google.colab import files
files.download('fake_job_model.zip')
print("Done! Check your downloads folder.")
