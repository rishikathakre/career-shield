{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT/DistilBERT Model Training\n",
        "## Fake Job Posting Detection\n",
        "\n",
        "This notebook fine-tunes a BERT/DistilBERT model for fake job posting detection:\n",
        "- Uses pre-trained DistilBERT (faster than BERT)\n",
        "- Fine-tunes on our dataset\n",
        "- Evaluates and compares with baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accelerate version: 1.12.0\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# Set project root\n",
        "project_root = Path(r'D:\\Data 641 NLP\\Final Project').resolve()\n",
        "os.chdir(project_root)\n",
        "sys.path.append(str(project_root / 'src'))\n",
        "\n",
        "# Install/verify required packages\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "try:\n",
        "    import accelerate\n",
        "    print(f\"accelerate version: {accelerate.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing accelerate...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"accelerate>=0.26.0\", \"-q\"])\n",
        "    import accelerate\n",
        "    print(f\"accelerate installed: {accelerate.__version__}\")\n",
        "\n",
        "from bert_model import BERTModel\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data splits loaded:\n",
            "Train: 12516 samples\n",
            "  - Real: 11910, Fake: 606\n",
            "\n",
            "Validation: 2682 samples\n",
            "  - Real: 2552, Fake: 130\n",
            "\n",
            "Test: 2682 samples\n",
            "  - Real: 2552, Fake: 130\n"
          ]
        }
      ],
      "source": [
        "# Load processed data splits\n",
        "train_df = pd.read_csv('data/processed/train.csv')\n",
        "val_df = pd.read_csv('data/processed/val.csv')\n",
        "test_df = pd.read_csv('data/processed/test.csv')\n",
        "\n",
        "print(\"Data splits loaded:\")\n",
        "print(f\"Train: {len(train_df)} samples\")\n",
        "print(f\"  - Real: {len(train_df[train_df['fraudulent']==0])}, Fake: {len(train_df[train_df['fraudulent']==1])}\")\n",
        "print(f\"\\nValidation: {len(val_df)} samples\")\n",
        "print(f\"  - Real: {len(val_df[val_df['fraudulent']==0])}, Fake: {len(val_df[val_df['fraudulent']==1])}\")\n",
        "print(f\"\\nTest: {len(test_df)} samples\")\n",
        "print(f\"  - Real: {len(test_df[test_df['fraudulent']==0])}, Fake: {len(test_df[test_df['fraudulent']==1])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 12516\n",
            "Validation samples: 2682\n",
            "Test samples: 2682\n"
          ]
        }
      ],
      "source": [
        "# Extract text and labels\n",
        "X_train = train_df['combined_text']\n",
        "y_train = train_df['fraudulent']\n",
        "\n",
        "X_val = val_df['combined_text']\n",
        "y_val = val_df['fraudulent']\n",
        "\n",
        "X_test = test_df['combined_text']\n",
        "y_test = test_df['fraudulent']\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize BERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized: DistilBERT\n",
            "This will download the pre-trained model on first run.\n"
          ]
        }
      ],
      "source": [
        "# Initialize DistilBERT model (faster than BERT, good performance)\n",
        "# Can change to 'bert-base-uncased' for better accuracy but slower training\n",
        "model = BERTModel(model_name='distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "print(\"Model initialized: DistilBERT\")\n",
        "print(\"This will download the pre-trained model on first run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing/upgrading transformers[torch] and accelerate...\n",
            "accelerate version: 1.12.0\n",
            "transformers version: 4.57.3\n",
            "Python executable: d:\\Data 641 NLP\\Final Project\\venv\\Scripts\\python.exe\n"
          ]
        }
      ],
      "source": [
        "# Install/verify required packages\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "print(\"Installing/upgrading transformers[torch] and accelerate...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers[torch]\", \"accelerate>=0.26.0\", \"-q\", \"--upgrade\"])\n",
        "\n",
        "# Reload modules\n",
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "print(f\"accelerate version: {accelerate.__version__}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "\n",
        "# Reload bert_model to pick up the new packages\n",
        "import importlib\n",
        "if 'bert_model' in sys.modules:\n",
        "    importlib.reload(sys.modules['bert_model'])\n",
        "from bert_model import BERTModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train BERT Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Use Google Colab with Free GPU\n",
        "\n",
        "If training is too slow on your machine, you can:\n",
        "1. Upload this notebook to Google Colab\n",
        "2. Enable free GPU: Runtime → Change runtime type → GPU\n",
        "3. Training will be 10-20x faster (15-30 minutes instead of hours)\n",
        "\n",
        "To use Colab:\n",
        "- Upload the notebook and data files\n",
        "- Install packages in first cell\n",
        "- Training will be much faster with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:bert_model:Preparing training dataset...\n",
            "INFO:bert_model:Preparing validation dataset...\n",
            "INFO:bert_model:accelerate version: 1.12.0\n",
            "INFO:bert_model:Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='243' max='2349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 243/2349 47:02 < 6:51:01, 0.09 it/s, Epoch 0.31/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the model\n",
        "# This may take 15-30 minutes depending on your hardware\n",
        "model.train(\n",
        "    train_texts=X_train,\n",
        "    train_labels=y_train,\n",
        "    val_texts=X_val,\n",
        "    val_labels=y_val,\n",
        "    output_dir='data/models/bert',\n",
        "    num_epochs=3,\n",
        "    batch_size=16,\n",
        "    learning_rate=2e-5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate on Validation Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "print(\"Validation Set Evaluation:\")\n",
        "val_metrics = model.evaluate(X_val, y_val)\n",
        "\n",
        "# Confusion matrix visualization\n",
        "y_val_pred = model.predict(X_val)\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.title('Confusion Matrix - Validation Set (BERT)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Test Set Evaluation:\")\n",
        "test_metrics = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Confusion matrix visualization\n",
        "y_test_pred = model.predict(X_test)\n",
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.title('Confusion Matrix - Test Set (BERT)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred, \n",
        "                          target_names=['Real', 'Fake']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare with Baseline Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline model for comparison\n",
        "from baseline_model import BaselineModel\n",
        "\n",
        "baseline = BaselineModel()\n",
        "baseline.load('data/models')\n",
        "baseline_test_metrics = baseline.evaluate(X_test, y_test)\n",
        "\n",
        "# Create comparison\n",
        "comparison = pd.DataFrame({\n",
        "    'Baseline (TF-IDF + LR)': [\n",
        "        baseline_test_metrics['accuracy'],\n",
        "        baseline_test_metrics['precision'],\n",
        "        baseline_test_metrics['recall'],\n",
        "        baseline_test_metrics['f1_score']\n",
        "    ],\n",
        "    'BERT/DistilBERT': [\n",
        "        test_metrics['accuracy'],\n",
        "        test_metrics['precision'],\n",
        "        test_metrics['recall'],\n",
        "        test_metrics['f1_score']\n",
        "    ]\n",
        "}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison.round(4))\n",
        "\n",
        "# Visualize comparison\n",
        "comparison.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Model Comparison: Baseline vs BERT')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metric')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"BERT Model Training Complete!\")\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
        "print(f\"  F1-Score: {test_metrics['f1_score']:.4f}\")\n",
        "\n",
        "print(f\"\\nModel saved to: data/models/bert/\")\n",
        "print(f\"\\nNext Steps:\")\n",
        "print(\"  1. Analyze errors and misclassifications\")\n",
        "print(\"  2. Implement impossible jobs detection feature\")\n",
        "print(\"  3. Build Streamlit dashboard\")\n",
        "print(\"  4. Final evaluation and report\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
