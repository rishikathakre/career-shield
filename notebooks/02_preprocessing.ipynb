{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing and Feature Engineering\n",
        "## Fake Job Posting Detection\n",
        "\n",
        "This notebook handles:\n",
        "- Text cleaning\n",
        "- Missing value handling\n",
        "- Feature engineering\n",
        "- Class imbalance handling\n",
        "- Train/validation/test splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: D:\\Data 641 NLP\\Final Project\n",
            "Current working directory: D:\\Data 641 NLP\\Final Project\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Get project root - use absolute path to project directory\n",
        "project_root = Path(r'D:\\Data 641 NLP\\Final Project').resolve()\n",
        "os.chdir(project_root)\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append(str(project_root / 'src'))\n",
        "from utils import clean_text, combine_text_features, calculate_text_statistics\n",
        "from data_preprocessing import DataPreprocessor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Raw Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: D:\\Data 641 NLP\\Final Project\\data\\raw\\fake_job_postings.csv\n",
            "File exists: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:utils:Loaded 17880 rows from D:\\Data 641 NLP\\Final Project\\data\\raw\\fake_job_postings.csv\n",
            "INFO:data_preprocessing:Loaded 17880 rows\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset shape: (17880, 18)\n",
            "\n",
            "Class distribution:\n",
            "fraudulent\n",
            "0    17014\n",
            "1      866\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class percentages:\n",
            "fraudulent\n",
            "0    95.1566\n",
            "1     4.8434\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = DataPreprocessor()\n",
        "\n",
        "# Use absolute path to ensure it works\n",
        "data_path = Path(project_root) / 'data' / 'raw' / 'fake_job_postings.csv'\n",
        "print(f\"Loading data from: {data_path}\")\n",
        "print(f\"File exists: {data_path.exists()}\")\n",
        "\n",
        "if not data_path.exists():\n",
        "    # Try alternative path\n",
        "    alt_path = Path('D:/Data 641 NLP/Final Project/data/raw/fake_job_postings.csv')\n",
        "    print(f\"\\nTrying alternative path: {alt_path}\")\n",
        "    print(f\"Alternative exists: {alt_path.exists()}\")\n",
        "    if alt_path.exists():\n",
        "        data_path = alt_path\n",
        "\n",
        "df = preprocessor.load_raw_data(str(data_path))\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['fraudulent'].value_counts().sort_index())\n",
        "print(f\"\\nClass percentages:\")\n",
        "print(df['fraudulent'].value_counts(normalize=True).sort_index() * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_preprocessing:Starting preprocessing...\n",
            "INFO:data_preprocessing:Cleaned title\n",
            "INFO:data_preprocessing:Cleaned description\n",
            "INFO:data_preprocessing:Cleaned company_profile\n",
            "INFO:data_preprocessing:Cleaned requirements\n",
            "INFO:data_preprocessing:Cleaned benefits\n",
            "INFO:data_preprocessing:Preprocessing complete!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed dataset shape: (17880, 27)\n",
            "\n",
            "New columns created: {'text_avg_sentence_length', 'text_word_count', 'has_requirements', 'text_char_count', 'has_benefits', 'text_avg_word_length', 'text_sentence_count', 'combined_text', 'has_company_profile'}\n"
          ]
        }
      ],
      "source": [
        "# Run preprocessing pipeline\n",
        "processed_df = preprocessor.preprocess()\n",
        "\n",
        "print(f\"Processed dataset shape: {processed_df.shape}\")\n",
        "print(f\"\\nNew columns created: {set(processed_df.columns) - set(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Handle Class Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution before handling imbalance:\n",
            "fraudulent\n",
            "0    17014\n",
            "1      866\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Real samples: 17014\n",
            "Fake samples: 866\n",
            "Imbalance ratio: 19.65:1\n"
          ]
        }
      ],
      "source": [
        "# Check class distribution after preprocessing\n",
        "print(\"Class distribution before handling imbalance:\")\n",
        "print(processed_df['fraudulent'].value_counts().sort_index())\n",
        "\n",
        "# We'll handle imbalance using:\n",
        "# 1. Class weights in models (already in baseline_model.py)\n",
        "# 2. Stratified splits (ensures balanced splits)\n",
        "# 3. Optional: Oversampling for training set\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_real = processed_df[processed_df['fraudulent'] == 0]\n",
        "df_fake = processed_df[processed_df['fraudulent'] == 1]\n",
        "\n",
        "print(f\"\\nReal samples: {len(df_real)}\")\n",
        "print(f\"Fake samples: {len(df_fake)}\")\n",
        "print(f\"Imbalance ratio: {len(df_real)/len(df_fake):.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Split Data into Train/Validation/Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_preprocessing:Train set: 12516 rows\n",
            "INFO:data_preprocessing:Validation set: 2682 rows\n",
            "INFO:data_preprocessing:Test set: 2682 rows\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split Summary:\n",
            "Train: 12516 samples\n",
            "  - Real: 11910, Fake: 606\n",
            "\n",
            "Validation: 2682 samples\n",
            "  - Real: 2552, Fake: 130\n",
            "\n",
            "Test: 2682 samples\n",
            "  - Real: 2552, Fake: 130\n"
          ]
        }
      ],
      "source": [
        "# Split data with stratification to maintain class distribution\n",
        "train_df, val_df, test_df = preprocessor.split_data(\n",
        "    test_size=0.15,\n",
        "    val_size=0.15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Split Summary:\")\n",
        "print(f\"Train: {len(train_df)} samples\")\n",
        "print(f\"  - Real: {len(train_df[train_df['fraudulent']==0])}, Fake: {len(train_df[train_df['fraudulent']==1])}\")\n",
        "print(f\"\\nValidation: {len(val_df)} samples\")\n",
        "print(f\"  - Real: {len(val_df[val_df['fraudulent']==0])}, Fake: {len(val_df[val_df['fraudulent']==1])}\")\n",
        "print(f\"\\nTest: {len(test_df)} samples\")\n",
        "print(f\"  - Real: {len(test_df[test_df['fraudulent']==0])}, Fake: {len(test_df[test_df['fraudulent']==1])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optional: Oversample Minority Class for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oversampling disabled. Will use class weights in models instead.\n"
          ]
        }
      ],
      "source": [
        "# Optionally oversample fake samples in training set\n",
        "# This increases fake samples from ~650 to match real samples\n",
        "# Note: We'll also use class weights in models, so this is optional\n",
        "\n",
        "oversample = False  # Set to True to enable oversampling\n",
        "\n",
        "if oversample:\n",
        "    train_real = train_df[train_df['fraudulent'] == 0]\n",
        "    train_fake = train_df[train_df['fraudulent'] == 1]\n",
        "    \n",
        "    # Upsample minority class\n",
        "    train_fake_upsampled = resample(\n",
        "        train_fake,\n",
        "        replace=True,\n",
        "        n_samples=len(train_real),\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Combine majority class with upsampled minority class\n",
        "    train_df_balanced = pd.concat([train_real, train_fake_upsampled])\n",
        "    train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"After oversampling:\")\n",
        "    print(f\"Train: {len(train_df_balanced)} samples\")\n",
        "    print(f\"  - Real: {len(train_df_balanced[train_df_balanced['fraudulent']==0])}, Fake: {len(train_df_balanced[train_df_balanced['fraudulent']==1])}\")\n",
        "    train_df = train_df_balanced\n",
        "else:\n",
        "    print(\"Oversampling disabled. Will use class weights in models instead.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:utils:Saved 12516 rows to data/processed/train.csv\n",
            "INFO:utils:Saved 2682 rows to data/processed/val.csv\n",
            "INFO:utils:Saved 2682 rows to data/processed/test.csv\n",
            "INFO:data_preprocessing:Processed data saved successfully!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data saved to data/processed/\n",
            "Files created:\n",
            "  - train.csv\n",
            "  - val.csv\n",
            "  - test.csv\n"
          ]
        }
      ],
      "source": [
        "# Save processed data splits\n",
        "preprocessor.save_processed_data(train_df, val_df, test_df)\n",
        "\n",
        "print(\"Processed data saved to data/processed/\")\n",
        "print(\"Files created:\")\n",
        "print(\"  - train.csv\")\n",
        "print(\"  - val.csv\")\n",
        "print(\"  - test.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Complete!\n",
            "\n",
            "Original dataset: 17,880 samples\n",
            "Processed dataset: 17,880 samples\n",
            "\n",
            "Data splits:\n",
            "  Train: 12,516 samples\n",
            "  Validation: 2,682 samples\n",
            "  Test: 2,682 samples\n",
            "\n",
            "Next steps:\n",
            "  1. Train baseline model (TF-IDF + Logistic Regression)\n",
            "  2. Fine-tune BERT model\n",
            "  3. Evaluate and compare models\n"
          ]
        }
      ],
      "source": [
        "print(\"Preprocessing Complete!\")\n",
        "print(f\"\\nOriginal dataset: {len(df):,} samples\")\n",
        "print(f\"Processed dataset: {len(processed_df):,} samples\")\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Train: {len(train_df):,} samples\")\n",
        "print(f\"  Validation: {len(val_df):,} samples\")\n",
        "print(f\"  Test: {len(test_df):,} samples\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(\"  1. Train baseline model (TF-IDF + Logistic Regression)\")\n",
        "print(\"  2. Fine-tune BERT model\")\n",
        "print(\"  3. Evaluate and compare models\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
